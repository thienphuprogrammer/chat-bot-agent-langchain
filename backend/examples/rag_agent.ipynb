{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:08:49.209060Z",
     "start_time": "2025-01-06T11:08:45.518461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]"
   ],
   "id": "6a68e13fcfb8e0f3",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:08:53.877092Z",
     "start_time": "2025-01-06T11:08:49.212525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=OllamaEmbeddings(model=\"llama3.2:1b\"),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ],
   "id": "9a2badd0a389a7a0",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:08:53.922607Z",
     "start_time": "2025-01-06T11:08:53.920381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_blog_posts\",\n",
    "    \"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.\",\n",
    ")\n",
    "\n",
    "tools = [retriever_tool]"
   ],
   "id": "8b707f488704e73b",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:08:53.965810Z",
     "start_time": "2025-01-06T11:08:53.964257Z"
    }
   },
   "cell_type": "code",
   "source": "from langchain_ollama import ChatOllama",
   "id": "44464bbfb0c6b5a7",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:08:54.035523Z",
     "start_time": "2025-01-06T11:08:54.007785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = ChatOllama(temperature=0, model=\"llama3.2:1b\")\n",
    "llm_with_tool = llm.bind_tools(tools)"
   ],
   "id": "d52bdc7e9b76c7ce",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:08:54.053368Z",
     "start_time": "2025-01-06T11:08:54.051747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field"
   ],
   "id": "bf17302d57305700",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:08:54.098133Z",
     "start_time": "2025-01-06T11:08:54.095577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Edges\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOllama(temperature=0, model=\"llama3.2:1b\")\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"rewrite\"\n"
   ],
   "id": "6e5ca2159772808c",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:08:54.142233Z",
     "start_time": "2025-01-06T11:08:54.140418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Nodes\n",
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    model = ChatOllama(temperature=0, model=\"llama3.2:1b\")\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n"
   ],
   "id": "69c2a54ba27a2893",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:08:54.361989Z",
     "start_time": "2025-01-06T11:08:54.184944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_agent = agent({\"messages\": [HumanMessage(content=\"What is the OmniPred?\")]})\n",
    "print(test_agent)"
   ],
   "id": "c8b1fbfef0332238",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "{'messages': [AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-01-06T11:08:54.359803687Z', 'done': True, 'done_reason': 'stop', 'total_duration': 148014965, 'load_duration': 9415726, 'prompt_eval_count': 187, 'prompt_eval_duration': 1000000, 'eval_count': 24, 'eval_duration': 136000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-1b31fcb9-432d-41eb-bef2-c8745723f393-0', tool_calls=[{'name': 'retrieve_blog_posts', 'args': {'query': 'OmniPred'}, 'id': '60b211a9-9321-4d54-8838-46fcf9a29021', 'type': 'tool_call'}], usage_metadata={'input_tokens': 187, 'output_tokens': 24, 'total_tokens': 211})]}\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:08:54.369682Z",
     "start_time": "2025-01-06T11:08:54.368276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "doc = \"\"\" \\n\n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n\n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question}\n",
    "    \\n ------- \\n\n",
    "Formulate an improved question: \"\"\"\n",
    "prompt_temp = ChatPromptTemplate.from_template(doc)"
   ],
   "id": "e0399202be401af5",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:08:54.417759Z",
     "start_time": "2025-01-06T11:08:54.415704Z"
    }
   },
   "cell_type": "code",
   "source": "prompt_temp.invoke({\"question\": \"What is the OmniPred?\"})",
   "id": "7657d9759e5bd314",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content=' \\n\\n    Look at the input and try to reason about the underlying semantic intent / meaning. \\n\\n    Here is the initial question:\\n    \\n ------- \\n\\n    What is the OmniPred?\\n    \\n ------- \\n\\nFormulate an improved question: ', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:08:54.460702Z",
     "start_time": "2025-01-06T11:08:54.458620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n\n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n\n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question}\n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    model = prompt_temp | ChatOllama(temperature=0, model=\"llama3.2:1b\")\n",
    "    response = model.invoke(question)\n",
    "    return {\"messages\": [response]}"
   ],
   "id": "ec1e068d09982827",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:08:54.973984Z",
     "start_time": "2025-01-06T11:08:54.505760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_rewrite = rewrite({\"messages\": [HumanMessage(content=\"What is the OmniPred?\")]})\n",
    "print(test_rewrite)"
   ],
   "id": "e4852c8d9f6e3d14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---TRANSFORM QUERY---\n",
      "{'messages': [AIMessage(content='Based on the context, I\\'m going to take a guess that \"OmniPred\" refers to a type of predator or hunter in various mythologies and cultures. With that in mind, here\\'s an improved question:\\n\\n\"What is the concept of the OmniPred?\"\\n\\nThis question still conveys the idea of something being a universal or omnipresent predator, but it\\'s more concise and focused on the underlying semantic intent.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-01-06T11:08:54.971443053Z', 'done': True, 'done_reason': 'stop', 'total_duration': 439309911, 'load_duration': 9106550, 'prompt_eval_count': 71, 'prompt_eval_duration': 1000000, 'eval_count': 84, 'eval_duration': 428000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-35fd6fb2-dbb3-424e-89f9-c9d99cc2cfdb-0', usage_metadata={'input_tokens': 71, 'output_tokens': 84, 'total_tokens': 155})]}\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:08:54.981670Z",
     "start_time": "2025-01-06T11:08:54.979789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOllama(temperature=0, model=\"llama3.2:1b\")\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}\n"
   ],
   "id": "3a3c3672c6815d3",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:08:55.024036Z",
     "start_time": "2025-01-06T11:08:55.020242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(\"agent\", agent)  # agent\n",
    "retrieve = ToolNode([retriever_tool])\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieval\n",
    "workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
    "workflow.add_node(\n",
    "    \"generate\", generate\n",
    ")  # Generating a response after we know the documents are relevant\n",
    "# Call agent node to decide to retrieve or not\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    # Assess agent decision\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()"
   ],
   "id": "d015252bca87ee52",
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:08:55.067398Z",
     "start_time": "2025-01-06T11:08:55.065720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from IPython.display import Image, display\n",
    "#\n",
    "# try:\n",
    "#     display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "# except Exception:\n",
    "#     # This requires some extra dependencies and is optional\n",
    "#     pass"
   ],
   "id": "8fb06ee59edf4871",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:08:56.779253Z",
     "start_time": "2025-01-06T11:08:55.109639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"Hello\"),\n",
    "    ]\n",
    "}\n",
    "output = graph.invoke(inputs)"
   ],
   "id": "c213d6249cdedf16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS RELEVANT---\n",
      "---GENERATE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/anaconda3/envs/LANGCHAIN/lib/python3.11/site-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T11:09:04.905925Z",
     "start_time": "2025-01-06T11:09:03.272552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pprint\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"What is the OmniPred?\"),\n",
    "    ]\n",
    "}\n",
    "for output in graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ],
   "id": "9c180a50893b0800",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "\"Output from node 'agent':\"\n",
      "'---'\n",
      "{ 'messages': [ AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-01-06T11:09:03.432821411Z', 'done': True, 'done_reason': 'stop', 'total_duration': 131369893, 'load_duration': 9112022, 'prompt_eval_count': 187, 'prompt_eval_duration': 1000000, 'eval_count': 20, 'eval_duration': 120000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-92174cf0-c7b3-4041-9ee7-f6be65cc2ac5-0', tool_calls=[{'name': 'retrieve_blog_posts', 'args': {'query': 'OmniPred'}, 'id': '17467ba3-1fb6-4f7e-a8d4-2aeecd50b959', 'type': 'tool_call'}], usage_metadata={'input_tokens': 187, 'output_tokens': 20, 'total_tokens': 207})]}\n",
      "'\\n---\\n'\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS RELEVANT---\n",
      "\"Output from node 'retrieve':\"\n",
      "'---'\n",
      "{ 'messages': [ ToolMessage(content='$$\\n\\n$$\\n\\n$$\\n\\n$$', name='retrieve_blog_posts', id='266e0815-2d2c-4d7a-bc0d-c861fef33a2d', tool_call_id='17467ba3-1fb6-4f7e-a8d4-2aeecd50b959')]}\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/anaconda3/envs/LANGCHAIN/lib/python3.11/site-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Output from node 'generate':\"\n",
      "'---'\n",
      "{'messages': [\"I don't know the OmniPred.\"]}\n",
      "'\\n---\\n'\n"
     ]
    }
   ],
   "execution_count": 120
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
