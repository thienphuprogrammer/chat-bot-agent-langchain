{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:05:28.826800Z",
     "start_time": "2025-01-02T18:05:28.825088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n"
   ],
   "id": "5d6e4855b829a403",
   "outputs": [],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:05:28.874105Z",
     "start_time": "2025-01-02T18:05:28.871100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "id": "e476d73f5e25c210",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:05:30.199386Z",
     "start_time": "2025-01-02T18:05:28.917083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from backend.utils import SplitterDocument\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "embedder = OllamaEmbeddings(model=\"llama3.2:1b\")\n",
    "model = OllamaLLM(model=\"llama3.2:1b\", temperature=0)"
   ],
   "id": "e8b5cfc5367b0a84",
   "outputs": [],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:05:30.205645Z",
     "start_time": "2025-01-02T18:05:30.204417Z"
    }
   },
   "cell_type": "code",
   "source": "splitter = SplitterDocument(chunk_size=300, chunk_overlap=30)",
   "id": "17c570a7635b75c2",
   "outputs": [],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:05:30.257228Z",
     "start_time": "2025-01-02T18:05:30.245517Z"
    }
   },
   "cell_type": "code",
   "source": "splits = splitter.splits(blog_docs)",
   "id": "a7c04fa7e3448cda",
   "outputs": [],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:05:32.471067Z",
     "start_time": "2025-01-02T18:05:30.289806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedder, collection_name=\"blog\",\n",
    "                                    persist_directory=\"./chroma_langchain_db\")"
   ],
   "id": "bdee857ecfe060a0",
   "outputs": [],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:05:32.477653Z",
     "start_time": "2025-01-02T18:05:32.476357Z"
    }
   },
   "cell_type": "code",
   "source": "retriever = vectorstore.as_retriever()",
   "id": "b02776504c4ffdc6",
   "outputs": [],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:05:32.519581Z",
     "start_time": "2025-01-02T18:05:32.518065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from typing import List\n",
    "from backend.common.config import BaseObject\n",
    "\n",
    "from backend.common.config import Config\n",
    "from backend.prompt import MULTI_TURN_PROMPT, FINAL_RAG_PROMPT\n"
   ],
   "id": "4b0f4a3050cea3df",
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:05:32.564125Z",
     "start_time": "2025-01-02T18:05:32.562017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_unique_union(documents: list[List]):\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "\n",
    "def _init_general_prompt_template(prompt_template: str = None):\n",
    "    prompt: ChatPromptTemplate = ChatPromptTemplate.from_template(prompt_template)\n",
    "    return prompt"
   ],
   "id": "e62677679f7f210",
   "outputs": [],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:05:32.609077Z",
     "start_time": "2025-01-02T18:05:32.605912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.tracers.langchain import wait_for_all_tracers\n",
    "\n",
    "\n",
    "class TranslationManager(BaseObject):\n",
    "    def __init__(\n",
    "            self,\n",
    "            config: Config = None,\n",
    "            model=None,\n",
    "            embedder=None,\n",
    "            retriever=None,\n",
    "            general_prompt_template: str = MULTI_TURN_PROMPT,\n",
    "            final_rag_prompt_template: str = FINAL_RAG_PROMPT,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config = config if config is not None else Config()\n",
    "        self._base_model = model\n",
    "        self._embedder = embedder\n",
    "        self._prompt_perspectives = _init_general_prompt_template(prompt_template=general_prompt_template)\n",
    "        self._prompt = _init_general_prompt_template(prompt_template=final_rag_prompt_template)\n",
    "        self._retriever = retriever\n",
    "        self._init_generate_queries()\n",
    "        self._init_retrieval_chain()\n",
    "        self._init_final_rag_chain()\n",
    "\n",
    "    def _init_generate_queries(self):\n",
    "        self._generate_queries = (\n",
    "                self._prompt_perspectives\n",
    "                | self._base_model\n",
    "                | StrOutputParser()\n",
    "                | (lambda x: x.split(\"\\n\"))  # Split by newlines\n",
    "                | (lambda x: [q for q in x if q])\n",
    "        ).with_config(run_name=\"TranslateResponse\")\n",
    "\n",
    "    def _init_retrieval_chain(self):\n",
    "        self._retrieval_chain = (\n",
    "                self._generate_queries\n",
    "                | self._retriever.map()\n",
    "                | get_unique_union\n",
    "        ).with_config(run_name=\"RetrieveResponse\")\n",
    "\n",
    "    def _init_final_rag_chain(self):\n",
    "        self._final_rag_chain = (\n",
    "                {\"context\": self._retrieval_chain,\n",
    "                 \"question\": itemgetter(\"question\")}\n",
    "                | self._prompt\n",
    "                | self._base_model\n",
    "                | StrOutputParser()\n",
    "        ).with_config(run_name=\"FinalRagChain\")\n",
    "\n",
    "    def predict(self, question):\n",
    "        try:\n",
    "            output = self._final_rag_chain.invoke({\"question\": question})\n",
    "            return output\n",
    "        finally:\n",
    "            wait_for_all_tracers()\n"
   ],
   "id": "6479f310eb93bb81",
   "outputs": [],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:05:54.962672Z",
     "start_time": "2025-01-02T18:05:50.995372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "translation_manager = TranslationManager(\n",
    "    model=model,\n",
    "    embedder=embedder,\n",
    "    retriever=retriever,\n",
    "    general_prompt_template=MULTI_TURN_PROMPT,\n",
    "    final_rag_prompt_template=FINAL_RAG_PROMPT)\n",
    "\n",
    "question = \"What is the speed of light?\"\n",
    "print(translation_manager.predict(question))"
   ],
   "id": "6a63b69ce83f3159",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question \"What is the speed of light?\" does not require any specific information or context from the provided text. It appears to be a general knowledge question that can be answered based on basic physics principles.\n",
      "\n",
      "The speed of light (approximately 299,792,458 meters per second) is a fundamental constant in physics and has been consistently measured and confirmed through various experiments over the years. There is no specific information or context in the provided text that would suggest otherwise.\n",
      "\n",
      "If you're looking for an answer to this question based on general knowledge, I can provide one: The speed of light is approximately 299,792,458 meters per second.\n"
     ]
    }
   ],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:05:37.359236Z",
     "start_time": "2025-01-02T18:05:37.356854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "different versions of the given user question to retrieve relevant documents from a vector\n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search.\n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "generate_queries = (\n",
    "        prompt_perspectives\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "        | (lambda x: x.split(\"\\n\"))  # Split by newlines\n",
    "        | (lambda x: [q for q in x if q])\n",
    ")"
   ],
   "id": "517bd35b194927",
   "outputs": [],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:05:38.303229Z",
     "start_time": "2025-01-02T18:05:37.399573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "qu = \"What is the speed of light?\"\n",
    "generate_queries.invoke({\"question\": qu})"
   ],
   "id": "5ba87a965eb60ea2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I can generate five different versions of the original question to retrieve relevant documents from a vector database. Here are the alternative questions:',\n",
       " 'What is the fundamental limit on the rate at which information can be transmitted through space?',\n",
       " \"What is the maximum speed at which any object or information can travel in a vacuum, as described by Einstein's theory of special relativity?\",\n",
       " 'How fast does light travel when it passes through a vacuum, according to the laws of physics and our current understanding of the universe?',\n",
       " 'What is the theoretical upper bound on the speed of electromagnetic radiation, including light, as predicted by quantum mechanics and general relativity?',\n",
       " \"Is there any known limit to the speed at which information can be transmitted through space, or is it an inherent property of the universe that we don't yet fully understand?\"]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:05:38.309897Z",
     "start_time": "2025-01-02T18:05:38.307814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n"
   ],
   "id": "7d3d2ddfa37327ba",
   "outputs": [],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:05:38.355465Z",
     "start_time": "2025-01-02T18:05:38.352658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retrieve\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union"
   ],
   "id": "a32b54ecb5428f68",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:05:39.428951Z",
     "start_time": "2025-01-02T18:05:38.398140Z"
    }
   },
   "cell_type": "code",
   "source": "docs = retrieval_chain.invoke({\"question\": question})",
   "id": "a81d6c414536d2d7",
   "outputs": [],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:05:39.436388Z",
     "start_time": "2025-01-02T18:05:39.434355Z"
    }
   },
   "cell_type": "code",
   "source": "docs",
   "id": "caf2aaecbde70c10",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Both TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:')]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T18:05:42.153660Z",
     "start_time": "2025-01-02T18:05:39.482804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "        {\"context\": retrieval_chain,\n",
    "         \"question\": itemgetter(\"question\")}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": question})"
   ],
   "id": "337e5597dccff849",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Task decomposition for LLM (Large Language Model) agents refers to the process of breaking down complex tasks into smaller, more manageable sub-tasks that can be executed by individual components or modules within the agent. This allows the agent to focus on one sub-task at a time and improve its overall performance.\\n\\nIn the context of Large Language Models, task decomposition is often used in conjunction with other techniques such as fine-tuning, instruction-based learning, and reinforcement learning. The goal of task decomposition is to enable LLMs to:\\n\\n1. Break down complex tasks into smaller sub-tasks\\n2. Focus on one sub-task at a time\\n3. Improve performance by executing each sub-task efficiently\\n\\nTask decomposition can be achieved through various techniques, including:\\n\\n1. **Fine-tuning**: Refining the model's parameters to focus on specific sub-tasks.\\n2. **Instruction-based learning**: Providing instructions or prompts that guide the agent to execute specific sub-tasks.\\n3. **Reinforcement learning**: Using feedback from the environment to improve performance and learn from experience.\\n\\nBy decomposing tasks into smaller sub-tasks, LLM agents can:\\n\\n1. Reduce computational complexity\\n2. Improve efficiency\\n3. Enhance overall performance\\n\\nTask decomposition is a crucial aspect of developing effective Large Language Models that can tackle complex tasks such as language translation, text summarization, and question answering.\""
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 145
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
